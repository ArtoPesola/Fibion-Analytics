---
title: "FibionFlow"
output: html_notebook
  toc: true
editor_options: 
  chunk_output_type: inline
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook including validated FibionFlow algorithms. 

When you execute code within the notebook, the results appear beneath the code. 

##Download the libraries to use

```{r}

#Zoo
if (!requireNamespace("zoo", quietly = TRUE)) {
  install.packages("zoo")
}
library(zoo)

#Plyr
if (!requireNamespace("plyr", quietly = TRUE)) {
  install.packages("plyr")
}
library(plyr)

#Tidyverse
if (!requireNamespace("tidyverse", quietly = TRUE)) {
  install.packages("tidyverse")
}
library(tidyverse)

#Readxl
if (!requireNamespace(c("readxl"), quietly = TRUE)) {
  install.packages(c("readxl"))
}
library(readxl)

#Writexl
if (!requireNamespace(c("writexl"), quietly = TRUE)) {
  install.packages(c("writexl"))
}
library(writexl)

#Data.table
if (!requireNamespace("data.table", quietly = TRUE)) {
  install.packages("data.table")
}
library(data.table)

#Here
if (!requireNamespace("here", quietly = TRUE)) {
  install.packages("here")
}
library(here)


```

##Set the working directory

```{r}

# Specify the relative path to the FibionFlow.Rmd file
here::set_here()

```

### Sleep / nonwear detection algorithm

```{r}

#First create lists where to save the output from the loop that goes through file list
activityresults_list = list()

#Get the list of files to be analysed. Allow .csv files with alternative writing format
fibionfiles <- list.files(path = file.path(here::here(), "Example data"), pattern="\\.(CSV|csv)$", recursive=TRUE, full.names=TRUE)

########## Set parameters for sleep-nonwear algorithm ##########

off_window_length <- 2  # Window length for "off" and "low variability" segments in hours
break_window_length <- 1  # Window length for break in "off" and "low variability" segments
activity_threshold <- 0.10  # Activity threshold for stopping the sleep/nonwear window
variability_threshold <- 0.8  # Threshold for low variability in all_METmin
sleep_window <- 15 # Define the sleep window in minutes (15 minutes before and after). This window is used to extend the off and low variability window in a stepwise manner
short_period_duration <- 4 # Define the duration of short wake periods marked as sleep/nonwear in hours
wake_criteria <- 10 # Define the criteria for minimal day length in hours

########## Set parameters for activity intensity classes ##########

light_threshold <- 3 # Light activity is less than 3 METs (excluding sitting)
mpa_threshold_min <- 3 # MPA is greater than or equal to 3 METs
mpa_threshold_max <- 6 # MPA is less than 6 METs
vpa_threshold <- 6  # VPA is greater than or equal to 6 METs

################################################################

#Loop over the file list to separate valid waking wear time from night and non-wear periods
for (currentFile in fibionfiles) {

# Read csv files
data_acc <- fread(currentFile, sep = ";", fill=T)

############# Data preparation #############

setnames(data_acc, old = c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9", "V10", "V11", "V12", "V13"), 
                 new = c("timestamp", "off_s", "sitting_s", "standing_s", "walking_s", "cycling_s", "high_intensity_s", "off_met100", "sitting_met100", "standing_met100", "walking_met100", "cycling_met100", "high_intensity_met100"))

# Initialize metadata_row to integer(0) to handle cases where [METADATA] is not found
metadata_row <- integer(0)

# Now, try to identify the row where '[METADATA]' starts
metadata_row <- which(data_acc$timestamp == "[METADATA]")

# If '[metadata]' is found, filter out all rows starting from that row
if (length(metadata_row) > 0) {
  data_acc <- data_acc[1:(metadata_row - 1)]
}

# Extract file information
file <- sub(".*data/", "", currentFile)
filename <- sub("\\.CSV$", "", file, ignore.case = TRUE)

# Ensure the timestamp format is okay
data_acc[, timestamp := as.POSIXct(timestamp)]

# Extract the hours and minutes component
data_acc[, time := format(timestamp, "%H:%M")]

# Extract the date component
data_acc[, date := as.Date(format(timestamp, "%Y-%m-%d"))]

# Create new variables weekday and week_weekend
data_acc[, c("weekday", "week_weekend") := list(weekdays(date), ifelse(weekdays(date) %in% c("Saturday", "Sunday"), "weekendday", "weekday"))]


setcolorder(data_acc, c("timestamp", "date", "time", "weekday", "week_weekend", names(data_acc)[!(names(data_acc) %in% c("timestamp", "date", "time", "weekday", "week_weekend"))]))

# Filter out rows that have more than 60 seconds of data
# Convert columns ending in "_s" to numeric
cols_to_convert <- names(data_acc)[grepl("_s$", names(data_acc))]
data_acc[, (cols_to_convert) := lapply(.SD, as.numeric), .SDcols = cols_to_convert]

data_acc[, rowsum_s := rowSums(.SD, na.rm = TRUE), .SDcols = patterns("_s$")]
#data_acc <- data_acc[rowsum_s <= 60]

############# Calculate METs at this stage because they are used in filtering low MET variability and later summarized #############

# Calculate METmin for each activity type
all_types <- c("off", "sitting", "standing", "walking", "cycling", "high_intensity")
activity_types <- c("standing", "walking", "cycling", "high_intensity")

# Calculate METmin for all types
for (activity in all_types) {
  
  met_col <- paste0(activity, "_met100")
  time_col <- paste0(activity, "_s")
  metmin_col <- paste0(activity, "_METmin")
  
  # Calculate MET minutes if not sitting
  data_acc[, (metmin_col) := get(met_col) / 100 * get(time_col) / 60]
  
}

# Calculate light, mpa and vpa for activity types, excluding off and sitting
for (activity in activity_types) {
  
  met_col <- paste0(activity, "_met100")
  time_col <- paste0(activity, "_s")
  metmin_col <- paste0(activity, "_METmin")
  light_col <- paste0(activity, "_light_min") # Light Activity, exclude sitting and off
  mpa_col <- paste0(activity, "_mpa_min") # Moderate Physical Activity, exclude sitting and off
  vpa_col <- paste0(activity, "_vpa_min") # Vigorous Physical Activity, exclude sitting and off
  
  # Calculate Light Activity minutes if not sitting
    data_acc[, (light_col) := fifelse(get(met_col)/100 < light_threshold, get(time_col) / 60, 0)]
  
  # Calculate MPA minutes if not sitting
  data_acc[, (mpa_col) := fifelse(get(met_col)/100 >= mpa_threshold_min & get(met_col)/100 < mpa_threshold_max, get(time_col) / 60, 0)]
  
  # Calculate VPA minutes if not sitting
  data_acc[, (vpa_col) := fifelse(get(met_col)/100 >= vpa_threshold, get(time_col) / 60, 0)]
  
}


# Calculate MET minutes for each activity type
all_metmin_cols <- paste0(all_types, "_METmin")
data_acc[, all_METmin := rowSums(.SD, na.rm = TRUE), .SDcols = all_metmin_cols]

# Calculate MET minutes for each activity type excluding 'sitting' and 'off'
activity_metmin_cols <- paste0(activity_types, "_METmin")
data_acc[, activity_METmin := rowSums(.SD, na.rm = TRUE), .SDcols = activity_metmin_cols]

# Calculate activity time
all_activity_cols <- paste0(activity_types, "_s")
data_acc[, activity_min := rowSums(.SD, na.rm = TRUE)/60, .SDcols = all_activity_cols]

# Compute overall walking METs threshold to use as a relative threshold
walking_METthreshold <- sum(data_acc$walking_METmin, na.rm = TRUE) / sum(data_acc$walking_s / 60, na.rm = TRUE)

#Calculate total activity duration and duration below and above walkingMETthreshold

# Iterate over each activity type for row-wise calculation
for (activity in activity_types) {
  met_col <- paste0(activity, "_met100")
  time_col <- paste0(activity, "_s")
  
  # Calculate durations row-wise. We need to first summarize METmin and then divide by time to account for different activity durations at different MET-levels
  data_acc[, (paste0(activity, "_below_threshold_min")) := fifelse(get(met_col) / 100 < walking_METthreshold, get(time_col) / 60, 0)]
  data_acc[, (paste0(activity, "_above_threshold_min")) := fifelse(get(met_col) / 100 >= walking_METthreshold, get(time_col) / 60, 0)]
  
}

# Accumulate the durations into the total columns, ignoring NA values
  data_acc[, duration_below_walkingMETs_min := rowSums(.SD, na.rm = TRUE), .SDcols = patterns("_below_threshold_min")]
  data_acc[, duration_above_walkingMETs_min := rowSums(.SD, na.rm = TRUE), .SDcols = patterns("_above_threshold_min")]


#Fill in the potential gaps in the timeline
  
# Define the desired interval (1 minute)
interval <- 60  # 60 minutes in seconds

# Create a complete sequence of timestamps
min_time <- min(data_acc$timestamp)
max_time <- max(data_acc$timestamp)
complete_timestamps <- seq(from = min_time, to = max_time, by = interval)

# Create a new data.table with complete timestamps
complete_data <- data.table(timestamp = complete_timestamps)

# Merge with the original data and carry the last observation forward
data_acc <- data_acc[complete_data, on = "timestamp", roll = "nearest"]



############# Sleep/nonwear detection #############

####### Step 1: Identify Off Segments as "SLNW" #######

# Add activity summary variables
data_acc[, Off := off_s]
data_acc[, Stationary := sitting_s + standing_s]
data_acc[, Activity := walking_s + cycling_s + high_intensity_s]

# Function to identify off and low variability segments
identify_segments <- function(data, off_window_length, break_window_length, activity_threshold, variability_threshold) {
    off_window_width <- off_window_length * 60  # Convert hours to minutes for the rolling window
    break_window_width <- break_window_length * 60 # Convert hours to minutes for the rolling window

    # Rolling sum of Off and Activity for the off_window_width
    data[, off_sum := rollapplyr(Off, width = off_window_width, FUN = sum, fill = NA, align = "center")]
    data[, activity_sum_next := shift(rollapplyr(Activity, width = break_window_width, FUN = sum, fill = NA, align = "center"), n = -1, fill = NA)]

    # Rolling variability of all_METmin for the off_window_width
    data[, roll_variability := rollapplyr(all_METmin, width = off_window_width, FUN = function(x) max(x, na.rm = TRUE) - min(x, na.rm = TRUE), fill = NA, align = "center")]

    # Define off_segment and low_variability based on conditions
    data[, off_segment := off_sum > 30 & (activity_sum_next / break_window_width <= activity_threshold * break_window_width)]
    data[, low_variability := roll_variability < variability_threshold]

    return(data)
}

# Apply the function to identify off and low variability segments
data_acc <- identify_segments(data_acc, off_window_length, break_window_length, activity_threshold, variability_threshold)

# Marking SLNW and Wake based on off and low variability segments
data_acc[, SLNW := ifelse(off_segment | low_variability, "SLNW", "Wake")]

# Clean up temporary columns
data_acc[, c("off_sum", "activity_sum_next", "off_segment", "low_variability", "roll_variability") := NULL]


####### Step 2: Iteratively extend the SLNW window #######

# Optimized Function to Identify and Update Surrounding Bouts
identify_surrounding_bouts_optimized <- function(data, sleep_window) {
    # Convert to data.table if it's not already
    setDT(data)

    # Create a grouping variable for consecutive SLNW segments
    data[, grp := cumsum(c(0L, diff(SLNW == "SLNW") != 0))]

    # Find start and end timestamps for each SLNW group
    slnw_groups <- data[SLNW == "SLNW", .(start_time = min(timestamp) - (sleep_window * 60), end_time = max(timestamp) + (sleep_window * 60)) , by = grp]

    # Loop through SLNW groups
    for (k in 1:nrow(slnw_groups)) {
        # Get the indices for the surrounding window of the group
        indices <- which(data$timestamp >= slnw_groups$start_time[k] & data$timestamp <= slnw_groups$end_time[k])
        
        # Apply the conditions to the surrounding window
        data[indices, SLNW := ifelse(
            (Stationary >= 2 * 60 * 60) |
            (SLNW == "SLNW") |
            ((Stationary >= 30 * 60) & (walking_s < 20)), 
            "SLNW", SLNW)]
    }

    # Drop the grouping variable
    data[, grp := NULL]
    
    return(data)
}

data_acc <- identify_surrounding_bouts_optimized(data_acc, sleep_window)


####### Step 3: Drop short wake periods if they are preceeded and followed by SLNW #######

# Calculate segment IDs
data_acc[, segment_id := rleid(SLNW)]

# Summarize the duration of each segment and calculate the proportion of Activity time
segment_summaries <- data_acc[, .(segment_duration = .N, 
                                  total_activity = sum(Activity)), by = .(segment_id, SLNW)]

# Calculate the proportion of Activity time in each segment
segment_summaries[, activity_ratio := total_activity / (segment_duration * 60)]  # Assuming each row represents 1 minute

# Shift to get previous and next segment durations
segment_summaries[, prev_duration := shift(segment_duration, n = 1, type = "lag", fill = 0)]
segment_summaries[, next_duration := shift(segment_duration, n = 1, type = "lead", fill = 0)]

# Join the segment summaries back to the original data
data_acc <- data_acc[segment_summaries, on = .(segment_id, SLNW)]

data_acc[, SLNW := ifelse(segment_duration < (short_period_duration * 60) &
                           SLNW == "Wake" &
                           activity_ratio <= activity_threshold &  # Maximum 10% Activity time
                           prev_duration >= 10 &
                           next_duration >= 10, 
                           "SLNW", SLNW)]


# Clean up temporary columns
data_acc[, c("Activity", "Stationary", "segment_duration", "total_activity", "activity_ratio", "prev_duration", "next_duration", "segment_id") := NULL]


####### Step 4: Validate Day Length #######

# Calculate SLNW duration per day
data_acc[, Wake_duration := sum(SLNW == "Wake", na.rm = TRUE), by = .(date)]  # Assuming each row is 1 minute

# Mark days as Short or Valid
data_acc[, Wake_length_criteria := ifelse(Wake_duration < wake_criteria * 60, "Short", "Valid"), by = .(date)]

# Mark SLNW segments for Short days
data_acc[Wake_length_criteria == "Short", SLNW := "SLNW"]


############ Calculate summarized outcomes by date for valid waking wear time ############

# Summarize data by date for SLNW == "Wake"
activitysummary <- data_acc[SLNW == "Wake", .(
  off_min = sum(off_s, na.rm = TRUE)/60,
  sitting_min = sum(sitting_s, na.rm = TRUE)/60,
  standing_min = sum(standing_s, na.rm = TRUE)/60,
  walking_min = sum(walking_s, na.rm = TRUE)/60,
  cycling_min = sum(cycling_s, na.rm = TRUE)/60,
  high_intensity_min = sum(high_intensity_s, na.rm = TRUE)/60,
  activity_min = sum(activity_min, na.rm=T),
  wake_duration_min = max(Wake_duration, na.rm = TRUE),
  off_METmin = sum(off_METmin, na.rm = TRUE),
  sitting_METmin = sum(sitting_METmin, na.rm = TRUE),
  standing_METmin = sum(standing_METmin, na.rm = TRUE),
  walking_METmin = sum(walking_METmin, na.rm = TRUE),
  cycling_METmin = sum(cycling_METmin, na.rm = TRUE),
  high_intensity_METmin = sum(high_intensity_METmin, na.rm = TRUE),
  all_METmin = sum(all_METmin, na.rm = TRUE),
  activity_METmin = sum(activity_METmin, na.rm = TRUE),
  light_min = sum(standing_light_min, walking_light_min, cycling_light_min, high_intensity_light_min, na.rm = TRUE),
  mpa_min = sum(standing_mpa_min, walking_mpa_min, cycling_mpa_min, high_intensity_mpa_min, na.rm = TRUE),
  vpa_min = sum(standing_vpa_min, walking_vpa_min, cycling_vpa_min, high_intensity_vpa_min, na.rm = TRUE),
  mvpa_min = sum(standing_mpa_min, walking_mpa_min, cycling_mpa_min, high_intensity_mpa_min, standing_vpa_min, walking_vpa_min, cycling_vpa_min, high_intensity_vpa_min, na.rm = TRUE),
  duration_below_walkingMETs_min = sum(duration_below_walkingMETs_min, na.rm = TRUE),
  duration_above_walkingMETs_min = sum(duration_above_walkingMETs_min, na.rm=TRUE)
), by = .(date)]

# Calculate percentages
activitysummary[, `:=` (
  off_percent = off_min / wake_duration_min * 100,
  sitting_percent = sitting_min / wake_duration_min * 100,
  standing_percent = standing_min / wake_duration_min * 100,
  walking_percent = walking_min / wake_duration_min * 100,
  cycling_percent = cycling_min / wake_duration_min * 100,
  high_intensity_percent = high_intensity_min / wake_duration_min * 100,
  activity_percent = activity_min / wake_duration_min * 100,
  light_percent = light_min / wake_duration_min * 100,
  mpa_percent = mpa_min / wake_duration_min * 100,
  vpa_percent = vpa_min / wake_duration_min * 100,
  mvpa_percent = mvpa_min / wake_duration_min * 100,
  duration_below_walkingMETs_percent = duration_below_walkingMETs_min / wake_duration_min * 100,
  duration_above_walkingMETs_percent = duration_above_walkingMETs_min / wake_duration_min * 100
)]

# Compute mean METs for each activity
activitysummary[, `:=` (off_METs = off_METmin / off_min,
                               sitting_METs = sitting_METmin / sitting_min,
                               standing_METs = standing_METmin / standing_min,
                               walking_METs = walking_METmin / walking_min,
                               cycling_METs = cycling_METmin / cycling_min,
                               high_intensity_METs = high_intensity_METmin / high_intensity_min,
                               all_METs = all_METmin / wake_duration_min,
                               activity_METs = activity_METmin / wake_duration_min)]


############ Calculate sitting and activity accumulation patterns ############

# Filter to include only valid waking wear time for analyses
data_bouts <- data_acc[SLNW == "Wake"]

# Calculate activity_sum and sittingbout1_activitybout0 within a single data.table operation
data_bouts[, `:=` (
  activity_sum = rowSums(.SD, na.rm = TRUE),
  sittingbout1_activitybout0 = fifelse(sitting_s == 60, "1", fifelse(rowSums(.SD, na.rm = TRUE) > 30, "0", NA_character_))
), .SDcols = c("standing_s", "walking_s", "cycling_s", "high_intensity_s")]

# Optionally, immediately remove activity_sum if it's no longer needed, to keep the data table clean.
data_bouts[, activity_sum := NULL]

# Filter out rows that are not sitting or activity bouts. These are occasions where the sitting or activity duration do not match the bout criterion.
data_bouts <- data_bouts[!is.na(sittingbout1_activitybout0), ]

# Generate a running number for unique bouts, within each 'date'
data_bouts[, bout_no := rleid(sittingbout1_activitybout0), by = .(date)]

# Calculate active bouts
data_bouts[, active_bout := ifelse(all_METmin > 1.5, 1, 0)]

# Calculate duration for each bout
data_bouts_summarized <- data_bouts[, .(bout_dur_min = as.numeric(.N), 
                                        activity_dur_min = sum(active_bout)), 
                                    by = .(date, sittingbout1_activitybout0, bout_no)]


# Aggregate to calculate bout counts and durations

# For sitting bouts
sitting_bouts_summary <- data_bouts_summarized[sittingbout1_activitybout0 == "1", .(
  
  # Count unique bouts
  sitting_bouts_n = uniqueN(bout_no),
  
  # Count the number of bouts in each duration category
  sitting_bouts_below10min_n = sum(bout_dur_min < 10),
  sitting_bouts_10to30min_n = sum(bout_dur_min >= 10 & bout_dur_min <= 30),
  sitting_bouts_above30min_n = sum(bout_dur_min > 30),
  
  # Sum the duration at each duration category
  sitting_bouts_below10min_dur_min = sum(bout_dur_min[bout_dur_min < 10], na.rm = TRUE),
  sitting_bouts_10to30min_dur_min = sum(bout_dur_min[bout_dur_min >= 10 & bout_dur_min <= 30], na.rm = TRUE),
  sitting_bouts_above30min_dur_min = sum(bout_dur_min[bout_dur_min > 30], na.rm = TRUE),
  
  # Summarize sitting bout durations
  sitting_bout_dur_min_mean = mean(bout_dur_min, na.rm = TRUE),
  sitting_bout_dur_min_median = median(bout_dur_min, na.rm = TRUE),
  
  # Calculate active sitting duration
  active_sitting_dur_min = sum(activity_dur_min, na.rm = TRUE),
  active_sitting_dur_percent = sum(activity_dur_min, na.rm = TRUE) / sum(bout_dur_min, na.rm = TRUE) * 100
  
), by = .(date, sittingbout1_activitybout0)]

# Clean up temporary columns
sitting_bouts_summary[, c("sittingbout1_activitybout0", "date") := NULL]

# For activity bouts
activity_bouts_summary <- data_bouts_summarized[sittingbout1_activitybout0 == "0", .(
  
  # Count unique bouts
  activity_bouts_n = uniqueN(bout_no),
  
  # Count the number of bouts in each duration category
  activity_bouts_below10min_n = sum(bout_dur_min < 10),
  activity_bouts_10to30min_n = sum(bout_dur_min >= 10 & bout_dur_min <= 30),
  activity_bouts_above30min_n = sum(bout_dur_min > 30),
  
  # Sum the duration at each duration category
  activity_bouts_below10min_dur_min = sum(bout_dur_min[bout_dur_min < 10], na.rm = TRUE),
  activity_bouts_10to30min_dur_min = sum(bout_dur_min[bout_dur_min >= 10 & bout_dur_min <= 30], na.rm = TRUE),
  activity_bouts_above30min_dur_min = sum(bout_dur_min[bout_dur_min > 30], na.rm = TRUE),
  
  # Summarize activity bout durations
  activity_bout_dur_min_mean = mean(bout_dur_min, na.rm = TRUE),
  activity_bout_dur_min_median = median(bout_dur_min, na.rm = TRUE)
), by = .(date, sittingbout1_activitybout0)]

activity_bouts_summary[, c("sittingbout1_activitybout0", "date") := NULL]


############ Combine total activity and bout summaries ############

dayresultssummary <- cbind(filename = filename, activitysummary, sitting_bouts_summary, activity_bouts_summary)

############ Calculate weekly means ############

# Create new variables weekday and week_weekend
dayresultssummary[, c("weekday", "week_weekend") := list(weekdays(date), ifelse(weekdays(date) %in% c("Saturday", "Sunday"), "weekendday", "weekday"))]
setcolorder(dayresultssummary, c("date", "weekday", "week_weekend", names(dayresultssummary)[!(names(dayresultssummary) %in% c("date", "weekday", "week_weekend"))]))

# Convert date to character to enable adding mean row
dayresultssummary$date <- as.character(dayresultssummary$date)

# Calculate means for numeric and integer columns only
numeric_means <- lapply(dayresultssummary[, .SD, .SDcols = sapply(dayresultssummary, is.numeric)], mean, na.rm = TRUE)

# Prepare a list for the new row
new_row_mean <- as.list(dayresultssummary[1,]) # Take the structure of the first row

# Replace the values in the new row with the means for numeric columns and "mean" for character columns
for (col_name in names(new_row_mean)) {
  if (is.numeric(new_row_mean[[col_name]])) {
    new_row_mean[[col_name]] <- numeric_means[[col_name]]
  } else if (col_name == "date") {
    new_row_mean[[col_name]] <- "mean"
  } else if (col_name == "filename") {
    new_row_mean[[col_name]] <- filename
  } else {
    new_row_mean[[col_name]] <- "mean"
  }
}

# Add the new row to the original data.table
dayresultssummary <- rbind(dayresultssummary, new_row_mean, fill = TRUE)

# Weighted mean calculation
weights <- ifelse(dayresultssummary$week_weekend == "weekday", 5/7, 2/7)
numeric_columns <- sapply(dayresultssummary, is.numeric)

# Calculate weighted means for numeric columns
numeric_weighted_means <- sapply(names(dayresultssummary)[numeric_columns], function(col_name) {
  sum(dayresultssummary[[col_name]] * weights, na.rm = TRUE) / sum(weights, na.rm = TRUE)
})

# Prepare a new row for the weighted means
new_row_weighted_mean <- new_row_mean # Start with the structure of the mean row

for (col_name in names(new_row_weighted_mean)) {
  if (is.numeric(new_row_weighted_mean[[col_name]])) {
    new_row_weighted_mean[[col_name]] <- numeric_weighted_means[[col_name]]
  } else if (col_name == "date") {
    new_row_weighted_mean[[col_name]] <- "weighted_mean"
  } else if (col_name == "filename") {
    new_row_weighted_mean[[col_name]] <- filename
  } else {
    new_row_weighted_mean[[col_name]] <- "weighted_mean"
  }
}


# Add the new row to the data.table
dayresultssummary <- rbind(dayresultssummary, new_row_weighted_mean, fill = TRUE)


############ Calculate usual bout durations ############

#Usual sitting bout duration

#Summarize data by bout length
sitting_bouts_summarized_arranged <- data_bouts_summarized %>% filter(sittingbout1_activitybout0 == "1") %>% group_by(bout_dur_min) %>% arrange((bout_dur_min)) %>% dplyr::summarize(bout_dur_min_sum = sum(bout_dur_min))

sitting_bouts_summarized_arranged2 <- sitting_bouts_summarized_arranged %>% dplyr::summarize(bout_dur_min = bout_dur_min, bout_durs_total_min = sum(bout_dur_min_sum), bout_durs_prop = bout_dur_min_sum/bout_durs_total_min, bout_durs_prop_sum = sum(bout_durs_prop), bout_durs_cumsum_min = cumsum(bout_durs_prop))

#First we define the function for the nonlinear model in order to iterate with several starting parameters
W50durs_sit_func <- function(bout_dur_min, sitW50dur_min, n) {
  I(bout_dur_min^n)/(I(bout_dur_min^n)+I(sitW50dur_min^n))
}

#Next we fit the model testing several starting parameters. This includes a catch function to return NA in case of error.
sitW50dur <- sitting_bouts_summarized_arranged2 %>%
  filter(n() > 5) %>%
  group_modify(~ {
    tryCatch(
      broom::tidy(
        nls.multstart::nls_multstart(
          bout_durs_cumsum_min ~ W50durs_sit_func(bout_dur_min, sitW50dur_min, n),
          data = .x,
          start_lower = c(sitW50dur_min = 0, n = 0.5),
          start_upper = c(sitW50dur_min = dayresultssummary$sitting_bout_dur_min_median[dayresultssummary$date == "mean"], n = 2),
          iter = 50,
          supp_errors = "Y"
        )
      ),
      error = function(e) tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA)
    )
  })

# Extract the estimate for sitW50dur_min
sitW50dur_min_estimate <- sitW50dur$estimate[sitW50dur$term == "sitW50dur_min"]

# Add a new column to dayresultssummary initialized with NA
dayresultssummary[, "sitW50dur_min" := NA_real_]

# Update the value for the row where date is "mean" with the correct column name
dayresultssummary[date == "mean", "sitW50dur_min" := sitW50dur_min_estimate]


#Usual activity bout duration

#Summarize data by bout length
activity_bouts_summarized_arranged <- data_bouts_summarized %>% filter(sittingbout1_activitybout0 == "0") %>% group_by(bout_dur_min) %>% arrange((bout_dur_min)) %>% dplyr::summarize(bout_dur_min_sum = sum(bout_dur_min))

activity_bouts_summarized_arranged2 <- activity_bouts_summarized_arranged %>% dplyr::summarize(bout_dur_min = bout_dur_min, bout_durs_total_min = sum(bout_dur_min_sum), bout_durs_prop = bout_dur_min_sum/bout_durs_total_min, bout_durs_prop_sum = sum(bout_durs_prop), bout_durs_cumsum_min = cumsum(bout_durs_prop))

#First we define the function for the nonlinear model in order to iterate with several starting parameters
W50durs_act_func <- function(bout_dur_min, actW50dur_min, n) {
  I(bout_dur_min^n)/(I(bout_dur_min^n)+I(actW50dur_min^n))
}

#Next we fit the model testing several starting parameters. This includes a catch function to return NA in case of error.
actW50dur <- activity_bouts_summarized_arranged2 %>% filter(n()>5) %>% 
  group_modify(~ {
    tryCatch(broom::tidy(nls.multstart::nls_multstart(bout_durs_cumsum_min ~ W50durs_act_func(bout_dur_min, actW50dur_min, n),
                             data = .x,
                             start_lower = c(actW50dur_min=0, n=0.5),
                             start_upper = c(actW50dur_min=dayresultssummary$activity_bout_dur_min_median[dayresultssummary$date=="mean"], n=2),
                             iter = 50,
                             supp_errors = "Y"
                             )
                         ),
      error = function(e) tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA)
    )
  })

# Extract the estimate for sitW50dur_min
actW50dur_min_estimate <- actW50dur$estimate[actW50dur$term == "actW50dur_min"]

# Add a new column to dayresultssummary initialized with NA
dayresultssummary[, "actW50dur_min" := NA_real_]

# Update the value for the row where date is "mean" with the correct column name
dayresultssummary[date == "mean", "actW50dur_min" := actW50dur_min_estimate]


############# Visualisation ############# 

#Create algorithm timestamp dataframe
# Create a column to identify changes in SLNW status
data_acc[, SLNW_change := ifelse(SLNW == "Wake", rleid(SLNW), NA)]

# Summarize data to get the first and last timestamp of each SLNW_change segment
data_acc_slnw <- data_acc %>%
  dplyr::filter(SLNW %in% c("Wake")) %>%  # Filter only relevant SLNW statuses
  group_by(SLNW_change, date) %>%
  dplyr::summarize(date = first(date),
    from = first(timestamp),  # First timestamp within each group
    to = last(timestamp)      # Last timestamp within each group
  ) %>%
  dplyr::filter(!is.na(SLNW_change))  # Filter out NA SLNW_change if needed

#Add ID t build a list for sensitivity & specificity analyses
data_acc_slnw$filename <- filename

# Preparing data for the plot of activity types
activity_data <- melt(data_acc, id.vars = c("date", "time", "timestamp"), measure.vars = c("off_s", "sitting_s", "standing_s", "walking_s", "cycling_s", "high_intensity_s"))

# Ensure 'variable' is a character to avoid factor-related issues
activity_data$variable <- as.character(activity_data$variable)

# Rename
activity_data <- activity_data %>%
  dplyr::mutate(variable = case_when(
    variable == "off_s" ~ "Off",
    variable == "sitting_s" ~ "Sitting",
    variable == "standing_s" ~ "Standing",
    variable == "walking_s" ~ "Walking",
    variable == "cycling_s" ~ "Cycling",
    variable == "high_intensity_s" ~ "High-intensity",
    TRUE ~ variable
  ))

activity_data$variable <- factor(activity_data$variable, levels = c("Off", "Sitting", "Standing", "Walking", "Cycling", "High-intensity"))

# Create the stacked bar plot with modifications for hours and minutes display
plot_activity_stacked <- ggplot() +
  geom_col(data=activity_data, aes(x = timestamp, y = value, fill = variable)) +
  geom_segment(data = data_acc_slnw, aes(x = from, xend = to, y = -5, yend = -5), color = "red") +
  scale_x_datetime(labels = function(x) format(x, "%H:%M"), date_breaks = "2 hours") +  # Adjusted for hours and minutes
  labs(title = "Activity Overview on Valid Waking Hours", x = "Date and Time", y = "Duration within one minute window (s)") +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 1), legend.position = "right") +
  facet_wrap(~date, ncol=1, scales="free_x") +
  scale_fill_manual(values = c(
    "Off" = "#F2F2F2",
    "Sitting" = "#EF4DB7",
    "Standing" = "#19D5FD",
    "Walking" = "#3399FF",
    "Cycling" = "#264FCD",
    "High-intensity" = "#333399"
  ))

plot_activity_stacked

# Name the plot
wd <- getwd()
file_name <- paste0(wd, "/Summaryplot_algorithm_", filename, ".pdf")

  # Save the plot
  ggsave(filename = file_name, plot = plot_activity_stacked, width = 10, height = 30, units = "in", dpi = 300)
  

activityresults_list[[currentFile]] <- dayresultssummary


}


activityresults_all = do.call(rbind.fill, activityresults_list)

#Write results as a text file
write.table(activityresults_all, "fibion_daily_summaries.txt")

#Write results as an excel file
write_xlsx(activityresults_all, "fibion_daily_summaries.xlsx")



```

## Custom events-based analysis

```{r}

#First create lists where to save the output from the loop that goes through file list
activityeventresults_list = list()

#Get the list of files to be analysed. Allow .csv files with alternative writing format
fibionfiles <- list.files(path = file.path(here::here(), "Example data"), pattern="\\.(CSV|csv)$", recursive=TRUE, full.names=TRUE)

########## Set parameters for activity intensity classes ##########

light_threshold <- 3 # Light activity is less than 3 METs (excluding sitting)
mpa_threshold_min <- 3 # MPA is greater than or equal to 3 METs
mpa_threshold_max <- 6 # MPA is less than 6 METs
vpa_threshold <- 6  # VPA is greater than or equal to 6 METs

################################################################

#Loop over the file list to separate valid waking wear time from night and non-wear periods
for (currentFile in fibionfiles) {
  
# Read csv files
data_acc <- fread(currentFile, sep = ";", fill=T)

############# Data preparation #############

setnames(data_acc, old = c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9", "V10", "V11", "V12", "V13"), 
                 new = c("timestamp", "off_s", "sitting_s", "standing_s", "walking_s", "cycling_s", "high_intensity_s", "off_met100", "sitting_met100", "standing_met100", "walking_met100", "cycling_met100", "high_intensity_met100"))

# Initialize metadata_row to integer(0) to handle cases where [METADATA] is not found
metadata_row <- integer(0)

# Now, try to identify the row where '[METADATA]' starts
metadata_row <- which(data_acc$timestamp == "[METADATA]")

# If '[metadata]' is found, filter out all rows starting from that row
if (length(metadata_row) > 0) {
  data_acc <- data_acc[1:(metadata_row - 1)]
}

# Extract file information
file <- sub(".*data/", "", currentFile)
filename <- sub("\\.CSV$", "", file, ignore.case = TRUE)

# Ensure the timestamp format is okay
data_acc[, timestamp := as.POSIXct(timestamp)]

# Set timezone to UTC without changing the time
attr(data_acc$timestamp, "tzone") <- "UTC"

# Get the data_acc timestamp
data_acc_timezone <- attr(data_acc$timestamp, "tzone")

# Extract the hours and minutes component
data_acc[, time := format(timestamp, "%H:%M")]

# Extract the date component
data_acc[, date := as.Date(format(timestamp, "%Y-%m-%d"))]

# Create new variables weekday and week_weekend
data_acc[, c("weekday", "week_weekend") := list(weekdays(date), ifelse(weekdays(date) %in% c("Saturday", "Sunday"), "weekendday", "weekday"))]

setcolorder(data_acc, c("timestamp", "date", "time", "weekday", "week_weekend", names(data_acc)[!(names(data_acc) %in% c("timestamp", "date", "time", "weekday", "week_weekend"))]))

# Filter out rows that have more than 60 seconds of data
# Convert columns ending in "_s" to numeric
cols_to_convert <- names(data_acc)[grepl("_s$", names(data_acc))]
data_acc[, (cols_to_convert) := lapply(.SD, as.numeric), .SDcols = cols_to_convert]

data_acc[, rowsum_s := rowSums(.SD, na.rm = TRUE), .SDcols = patterns("_s$")]
#data_acc <- data_acc[rowsum_s <= 60]

############# Get the events file #############

# Load the Excel file and match the sheet current file
events <- read_excel(path = file.path(here::here(), "Example data/events.xlsx"), sheet=file)

setDT(events)

# Make sure events timestamp is in PosixCT format
events <- events %>% 
mutate(from = as.POSIXct(from, format = "%d.%m.%Y %H:%M"),
to = as.POSIXct(to, format = "%d.%m.%Y %H:%M"))

eventstimezone <- attr(events$from, "tzone")

############# Calculate METs #############

# Calculate METmin for each activity type
all_types <- c("off", "sitting", "standing", "walking", "cycling", "high_intensity")
activity_types <- c("standing", "walking", "cycling", "high_intensity")

# Calculate METmin for all types
for (activity in all_types) {
  
  met_col <- paste0(activity, "_met100")
  time_col <- paste0(activity, "_s")
  metmin_col <- paste0(activity, "_METmin")
  
  # Calculate MET minutes if not sitting
  data_acc[, (metmin_col) := get(met_col) / 100 * get(time_col) / 60]
  
}

# Calculate light, mpa and vpa for activity types, excluding off and sitting
for (activity in activity_types) {
  
  met_col <- paste0(activity, "_met100")
  time_col <- paste0(activity, "_s")
  metmin_col <- paste0(activity, "_METmin")
  light_col <- paste0(activity, "_light_min") # Light Activity, exclude sitting and off
  mpa_col <- paste0(activity, "_mpa_min") # Moderate Physical Activity, exclude sitting and off
  vpa_col <- paste0(activity, "_vpa_min") # Vigorous Physical Activity, exclude sitting and off
  
  # Calculate Light Activity minutes if not sitting
    data_acc[, (light_col) := fifelse(get(met_col)/100 < light_threshold, get(time_col) / 60, 0)]
  
  # Calculate MPA minutes if not sitting
  data_acc[, (mpa_col) := fifelse(get(met_col)/100 >= mpa_threshold_min & get(met_col)/100 < mpa_threshold_max, get(time_col) / 60, 0)]
  
  # Calculate VPA minutes if not sitting
  data_acc[, (vpa_col) := fifelse(get(met_col)/100 >= vpa_threshold, get(time_col) / 60, 0)]
  
}


# Calculate MET minutes for each activity type
all_metmin_cols <- paste0(all_types, "_METmin")
data_acc[, all_METmin := rowSums(.SD, na.rm = TRUE), .SDcols = all_metmin_cols]

# Calculate MET minutes for each activity type excluding 'sitting' and 'off'
activity_metmin_cols <- paste0(activity_types, "_METmin")
data_acc[, activity_METmin := rowSums(.SD, na.rm = TRUE), .SDcols = activity_metmin_cols]

# Calculate activity time
all_activity_cols <- paste0(activity_types, "_s")
data_acc[, activity_min := rowSums(.SD, na.rm = TRUE)/60, .SDcols = all_activity_cols]

# Compute overall walking METs threshold to use as a relative threshold
walking_METthreshold <- sum(data_acc$walking_METmin, na.rm = TRUE) / sum(data_acc$walking_s / 60, na.rm = TRUE)

#Calculate total activity duration and duration below and above walkingMETthreshold

# Iterate over each activity type for row-wise calculation
for (activity in activity_types) {
  metmin_col <- paste0(activity, "_METmin")
  time_col <- paste0(activity, "_s")
  
  # Calculate durations row-wise. We need to first summarize METmin and then divide by time to account for different activity durations at different MET-levels
  data_acc[, (paste0(activity, "_below_threshold_min")) := fifelse(get(met_col)/100 < walking_METthreshold, get(time_col) / 60, 0)]
  data_acc[, (paste0(activity, "_above_threshold_min")) := fifelse(get(met_col)/100 >= walking_METthreshold, get(time_col) / 60, 0)]
  
}

# Accumulate the durations into the total columns, ignoring NA values
  data_acc[, duration_below_walkingMETs_min := rowSums(.SD, na.rm = TRUE), .SDcols = patterns("_below_threshold_min")]
  data_acc[, duration_above_walkingMETs_min := rowSums(.SD, na.rm = TRUE), .SDcols = patterns("_above_threshold_min")]


#Fill in the potential gaps in the timeline
  
# Define the desired interval (1 minute)
interval <- 60  # 60 minutes in seconds

# Create a complete sequence of timestamps
min_time <- min(data_acc$timestamp)
max_time <- max(data_acc$timestamp)
complete_timestamps <- seq(from = min_time, to = max_time, by = interval)

# Create a new data.table with complete timestamps
complete_data <- data.table(timestamp = complete_timestamps)

# Merge with the original data and carry the last observation forward
data_acc <- data_acc[complete_data, on = "timestamp", roll = "nearest"]


############ Calculate summarized outcomes by custom events ############

# Create a new column in data_acc to store the corresponding event
data_acc[, event := NA_character_]

# For each row in events, mark the corresponding rows in data_acc with the event
for (i in seq_len(nrow(events))) {
  data_acc[timestamp >= events$from[i] & timestamp <= events$to[i], event := events$event[i]]
}

#Filter out rows that do not belong to any events
data_acc_filtered <- data_acc[!is.na(event), ]

# Summarize data by event
activitysummary_events <- data_acc_filtered[, .(
  off_min = sum(off_s, na.rm = TRUE)/60,
  sitting_min = sum(sitting_s, na.rm = TRUE)/60,
  standing_min = sum(standing_s, na.rm = TRUE)/60,
  walking_min = sum(walking_s, na.rm = TRUE)/60,
  cycling_min = sum(cycling_s, na.rm = TRUE)/60,
  high_intensity_min = sum(high_intensity_s, na.rm = TRUE)/60,
  activity_min = sum(activity_min, na.rm=T),
  duration_min = .N,
  off_METmin = sum(off_METmin, na.rm = TRUE),
  sitting_METmin = sum(sitting_METmin, na.rm = TRUE),
  standing_METmin = sum(standing_METmin, na.rm = TRUE),
  walking_METmin = sum(walking_METmin, na.rm = TRUE),
  cycling_METmin = sum(cycling_METmin, na.rm = TRUE),
  high_intensity_METmin = sum(high_intensity_METmin, na.rm = TRUE),
  all_METmin = sum(all_METmin, na.rm = TRUE),
  activity_METmin = sum(activity_METmin, na.rm = TRUE),
  light_min = sum(standing_light_min, walking_light_min, cycling_light_min, high_intensity_light_min, na.rm = TRUE),
  mpa_min = sum(standing_mpa_min, walking_mpa_min, cycling_mpa_min, high_intensity_mpa_min, na.rm = TRUE),
  vpa_min = sum(standing_vpa_min, walking_vpa_min, cycling_vpa_min, high_intensity_vpa_min, na.rm = TRUE),
  mvpa_min = sum(standing_mpa_min, walking_mpa_min, cycling_mpa_min, high_intensity_mpa_min, standing_vpa_min, walking_vpa_min, cycling_vpa_min, high_intensity_vpa_min, na.rm = TRUE),
  duration_below_walkingMETs_min = sum(duration_below_walkingMETs_min, na.rm = TRUE),
  duration_above_walkingMETs_min = sum(duration_above_walkingMETs_min, na.rm=TRUE)
), 
    by = .(date, event)]

# Calculate percentages
activitysummary_events[, `:=` (
  off_percent = off_min / duration_min * 100,
  sitting_percent = sitting_min / duration_min * 100,
  standing_percent = standing_min / duration_min * 100,
  walking_percent = walking_min / duration_min * 100,
  cycling_percent = cycling_min / duration_min * 100,
  high_intensity_percent = high_intensity_min / duration_min * 100,
  activity_percent = activity_min / duration_min * 100,
  light_percent = light_min / duration_min * 100,
  mpa_percent = mpa_min / duration_min * 100,
  vpa_percent = vpa_min / duration_min * 100,
  mvpa_percent = mvpa_min / duration_min * 100,
  duration_below_walkingMETs_percent = duration_below_walkingMETs_min / duration_min * 100,
  duration_above_walkingMETs_percent = duration_above_walkingMETs_min / duration_min * 100
)]

# Compute mean METs for each activity
activitysummary_events[, `:=` (off_METs = off_METmin / off_min,
                               sitting_METs = sitting_METmin / sitting_min,
                               standing_METs = standing_METmin / standing_min,
                               walking_METs = walking_METmin / walking_min,
                               cycling_METs = cycling_METmin / cycling_min,
                               high_intensity_METs = high_intensity_METmin / high_intensity_min,
                               all_METs = all_METmin / duration_min,
                               activity_METs = activity_METmin / duration_min)]


############ Calculate sitting and activity accumulation patterns ############

# Calculate activity_sum and sittingbout1_activitybout0 within a single data.table operation
data_acc_filtered[, `:=` (
  activity_sum = rowSums(.SD, na.rm = TRUE),
  sittingbout1_activitybout0 = fifelse(sitting_s == 60, "1", fifelse(rowSums(.SD, na.rm = TRUE) > 30, "0", NA_character_))
), .SDcols = c("standing_s", "walking_s", "cycling_s", "high_intensity_s")]

# Optionally, immediately remove activity_sum if it's no longer needed, to keep the data table clean.
data_acc_filtered[, activity_sum := NULL]

# Filter out rows that are not sitting or activity bouts. These are occasions where the sitting or activity duration do not match the bout criterion.
data_acc_filtered_bouts <- data_acc_filtered[!is.na(sittingbout1_activitybout0), ]

# Generate a running number for unique bouts, within each 'date'
data_acc_filtered_bouts[, bout_no := rleid(sittingbout1_activitybout0), by = .(date, event)]

# Calculate duration for each bout
data_bouts_events_summarized <- data_acc_filtered_bouts[, .(
  bout_dur_min = .N
), by = .(date, event, sittingbout1_activitybout0, bout_no)]

# Aggregate to calculate bout counts and durations

# For sitting bouts
sitting_bouts_events_summary <- data_bouts_events_summarized[sittingbout1_activitybout0 == "1", .(
  
  # Count unique bouts
  sitting_bouts_n = uniqueN(bout_no),
  
  # Count the number of bouts in each duration category
  sitting_bouts_below10min_n = sum(bout_dur_min < 10),
  sitting_bouts_10to30min_n = sum(bout_dur_min >= 10 & bout_dur_min <= 30),
  sitting_bouts_above30min_n = sum(bout_dur_min > 30),
  
  # Sum the duration at each duration category
  sitting_bouts_below10min_dur_min = sum(bout_dur_min[bout_dur_min < 10], na.rm = TRUE),
  sitting_bouts_10to30min_dur_min = sum(bout_dur_min[bout_dur_min >= 10 & bout_dur_min <= 30], na.rm = TRUE),
  sitting_bouts_above30min_dur_min = sum(bout_dur_min[bout_dur_min > 30], na.rm = TRUE),
  
  # Summarize sitting bout durations
  sitting_bout_dur_min_mean = mean(as.numeric(bout_dur_min), na.rm = TRUE),
  sitting_bout_dur_min_median = median(as.numeric(bout_dur_min), na.rm = TRUE)
), by = .(date, event, sittingbout1_activitybout0)]

# Clean up temporary columns
sitting_bouts_events_summary[, c("sittingbout1_activitybout0", "date") := NULL]

# For activity bouts
activity_bouts_events_summary <- data_bouts_events_summarized[sittingbout1_activitybout0 == "0", .(
  
  # Count unique bouts
  activity_bouts_n = uniqueN(bout_no),
  
  # Count the number of bouts in each duration category
  activity_bouts_below10min_n = sum(bout_dur_min < 10),
  activity_bouts_10to30min_n = sum(bout_dur_min >= 10 & bout_dur_min <= 30),
  activity_bouts_above30min_n = sum(bout_dur_min > 30),
  
  # Sum the duration at each duration category
  activity_bouts_below10min_dur_min = sum(bout_dur_min[bout_dur_min < 10], na.rm = TRUE),
  activity_bouts_10to30min_dur_min = sum(bout_dur_min[bout_dur_min >= 10 & bout_dur_min <= 30], na.rm = TRUE),
  activity_bouts_above30min_dur_min = sum(bout_dur_min[bout_dur_min > 30], na.rm = TRUE),
  
  # Summarize activity bout durations
  activity_bout_dur_min_mean = mean(as.numeric(bout_dur_min), na.rm = TRUE),
  activity_bout_dur_min_median = median(as.numeric(bout_dur_min), na.rm = TRUE)
), by = .(date, event, sittingbout1_activitybout0)]

activity_bouts_events_summary[, c("sittingbout1_activitybout0", "date") := NULL]


############ Combine total activity and bout summaries ############

eventresultssummary <- cbind(filename = filename, activitysummary_events, sitting_bouts_events_summary, activity_bouts_events_summary)


############# Visualisation ############# 

#Create algorithm timestamp dataframe
# Create a column to identify changes in SLNW status
data_acc[, event_change := ifelse(!is.na(event), rleid(event), NA)]

# Summarize data to get the first and last timestamp of each SLNW_change segment
data_acc_events <- data_acc %>%
  dplyr::filter(!is.na(event)) %>%  # Filter only events
  group_by(event_change, event, date) %>%
  dplyr::summarize(date = first(date),
    from = first(timestamp),  # First timestamp within each group
    to = last(timestamp)      # Last timestamp within each group
  ) %>%
  dplyr::filter(!is.na(event_change))  # Filter out NA SLNW_change if needed

#Add ID t build a list for sensitivity & specificity analyses
data_acc_events$filename <- filename

# Preparing data for the plot of activity types
activity_data <- melt(data_acc, id.vars = c("date", "time", "timestamp"), measure.vars = c("off_s", "sitting_s", "standing_s", "walking_s", "cycling_s", "high_intensity_s"))

# Ensure 'variable' is a character to avoid factor-related issues
activity_data$variable <- as.character(activity_data$variable)

# Rename
activity_data <- activity_data %>%
  dplyr::mutate(variable = case_when(
    variable == "off_s" ~ "Off",
    variable == "sitting_s" ~ "Sitting",
    variable == "standing_s" ~ "Standing",
    variable == "walking_s" ~ "Walking",
    variable == "cycling_s" ~ "Cycling",
    variable == "high_intensity_s" ~ "High-intensity",
    TRUE ~ variable
  ))

activity_data$variable <- factor(activity_data$variable, levels = c("Off", "Sitting", "Standing", "Walking", "Cycling", "High-intensity"))

# Create the stacked bar plot with modifications for hours and minutes display
plot_activity_stacked_events <- ggplot() +
  geom_col(data=activity_data, aes(x = timestamp, y = value, fill = variable)) +
  geom_segment(data = data_acc_events, aes(x = from, xend = to, y = -5, yend = -5, color = event)) +
  scale_x_datetime(labels = function(x) format(x, "%H:%M"), date_breaks = "2 hours") +  # Adjusted for hours and minutes
  labs(title = "Activity Overview on Valid Waking Hours", x = "Date and Time", y = "Duration within one minute window (s)") +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 1), legend.position = "right") +
  facet_wrap(~date, ncol=1, scales="free_x") +
  scale_fill_manual(values = c(
    "Off" = "#F2F2F2",
    "Sitting" = "#EF4DB7",
    "Standing" = "#19D5FD",
    "Walking" = "#3399FF",
    "Cycling" = "#264FCD",
    "High-intensity" = "#333399"
  ))

plot_activity_stacked_events

# Name the plot
wd <- getwd()
file_name <- paste0(wd, "/Summaryplot_events_", filename, ".pdf")

  # Save the plot
  ggsave(filename = file_name, plot = plot_activity_stacked_events, width = 10, height = 30, units = "in", dpi = 300)
  

activityeventresults_list[[currentFile]] <- eventresultssummary


}

activityeventresults_all = do.call(rbind.fill, activityeventresults_list)

#Write results as a text file
write.table(activityeventresults_all, "fibion_event_summaries.txt")

#Write results as an excel file
write_xlsx(activityeventresults_all, "fibion_event_summaries.xlsx")



```




```{r}


```



